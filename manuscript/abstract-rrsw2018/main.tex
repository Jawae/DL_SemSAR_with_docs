\documentclass[a4paper]{article}
\usepackage[a4paper, total={7in, 11in}]{geometry}

%\usepackage[square]{natbib}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[numbers]{natbib}

% For table next to image
\usepackage{floatrow}
% Table float box with bottom caption, box width adjusted to content
\newfloatcommand{capbtabbox}{table}[][\FBwidth]

\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{verbatim}

% for tables
\usepackage{multicol}

\DeclareFloatVCode{largevskip}%
{\vskip 20pt}
\floatsetup{captionskip=0pt,valign=l }%
\captionsetup[subfigure]{}

% for a row of figures next to each other
\usepackage{multicol}
\usepackage{lipsum}
\usepackage{mwe}

% to wrap text around the table
\usepackage{wrapfig,lipsum,booktabs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\pagenumbering{gobble}

\Large
 \begin{center}
Deep learning for semantic segmentation of SAR images\\ 

\hspace{10pt}

% Author names and affiliations
\large
Sanja \v{S}\'{c}epanovi\'{c}$^1$, Vladimir Ignatenko$^1$, Pekka Laurila$^1$ \\

\hspace{10pt}

\small  
$^1$ ICEYE\\
name.surname@iceye.fi

\end{center}

\hspace{10pt}

\normalsize
Recent advances in deep learning (DL) techniques for computer vision have lead to the application of DL in a number of fields that rely on computer vision. Example fields are autonomous driving, computational photography, image search engines, medical diagnostics, augmented reality, and remote sensing data analytics. In this short paper, we present our work-in-progress on the application of deep learning for synthetic aperture radar (SAR) image analytics. In particular, we apply the semantic segmentation DL algorithms towards the land cover segmentation. % and image scene understanding.


The tree classes of DL algorithms in computer vision are based on their main tasks. In the order of increasing complexity, they are:
\begin{itemize}
\itemsep0em 
\item \textbf{classification}: assigning an image to a class based on what is (mainly) represented in it, for example a ship, oil tank, sea or land;
\item \textbf{object detection}: detecting (and, in more advanced versions of the algorithms, localizing) presence of particular objects in an image. These algorithms can detect several objects in the given image. A notable example task in this category is ship detection in SAR images;
\item \textbf{semantic segmentation}: assigning a class to each \textit{pixel} in an image based on which class (image segment) it belongs to. These algorithms not only detect and localize objects in the image, but also output their exact areas and boundaries.
\end{itemize}

\thisfloatsetup{floatrowsep=none}
\begin{figure}[htb]
\begin{floatrow}

\capbtabbox{%
\begin{tabular}{ l r }
 \hline
	\textbf{Training} &  \\
  \hline			
  Source & Sentinel-1 GRD \\
  Polarization & VV \& VH  \\
  Res.\ SM & $23\times 23$m \\
  Res.\ IW & $20\times 22$m \\
  Count & $60\times 10,000$ \\
  \hline  
\end{tabular}
\begin{tabular}{ l r }
 \hline
	\textbf{Label} &  \\
  \hline			
  Source & Corine mask \\
  Res.\ & $20\times 20$m \\
  Fine classes & $48$ \\
  Top classes & $6$ \\
  Count & $600,000$ \\
  \hline  
\end{tabular}
}{%
  \caption{Datasets info}%
}
\ffigbox{%
  \includegraphics[width=0.25\linewidth]{fig/clc2012_fi20m.png}%
}{%
  \caption{Corine mask for Finland}%
}

\end{floatrow}
\end{figure}

DL algorithms require large datasets (on the scales of millions of images) and long training times. \textbf{Transfer learning} is an effective approach to tackle this issue by reusing DL networks pre-trained on available large datasets for a relatively similar task and then fine-tuning them at the smaller data at task. 

\paragraph{Related work} As discussed in the review of DL in remote sensing \cite{zhu2017deep}, the specificity of remote sensing imagery (compared to ordinary RGB or grayscale images) results in specific challenges in this area. For example, remote sensing data are georeferenced, often multi-modal, with particular imaging geometries, there are interpretation difficulties and the ground-truth or labeled data needed for DL are often still lacking. Nevertheless, a number of research papers tackling remote sensing imagery with DL techniques is already published. Object detection (and localization) is usually termed \textit{automatic target recognition} (ATR) in the context of remote sensing, and SAR-ATR when it comes to SAR images in particular \cite{7460942}.
%Below, we review those papers based on which of the three classes of DL algorithms they investigate. 
\citeauthor{7301382} \cite{7301382} evaluated the transfer learning from recognition of everyday objects to remote sensing objects and showed it is successful. For instance, transfer learning is employed to tackle the lack of labeled data in ATR \cite{huang2017transfer}. In general, there is already significant success in applying DL techniques for SAR-ATR and scene understanding. However, for the \textit{semantic image segmentation} and image preprocessing, advances are yet to be achieved \cite{zhang2016deep}.

\paragraph{Semantic segmentation of SAR images for creating the and cover mask}

\paragraph{Task} Automatically segment pixels of a SAR image based on the type of land cover they represent.

\paragraph{Datasets}
We analyze a set of Sentinel-1 SAR images (Level-1 Ground Range Detected -- GRD products in Stripmap (SM) or Interferometric Wide (IW) swath modes). For the labels, we use the existing Corine mask\footnote{https://land.copernicus.eu/pan-european/corine-land-cover}, providing mix of automatic and human experts labeled information for the type of land cover divided in over $40$ classes. Corine is a program by the EU and stands for \textit{coordination of information on the environment}. While a Corine mask of $100\times 100m$ spatial resolution is available for most of the EU territory, the Finnish Environment Institute created a more precise, $20\times 20m$ spatial resolution mask for Finland. We choose to work with the more precise, Finnish, mask, because its resolution corresponds better with the resolution of Sentinel-1 images, in particular, when using SM (strip map) and IW (interferometric wide swath) modes GRD products.

\paragraph{Hardware setup and preprocessing}
We develop and test DL models on a single GPU-machine (NVIDIA GeForce GTX 1080). Using this setup, the maximum image size we can work with is around $1000 \times 1000$ pixels (depending on the model) and so we split the images in a number of pieces accordingly. Moreover, the Sentinel-1 images come in two polarizations and so we try two setups for the training and testing data. The first setup we term \textbf{single-channel data} and both, VV and VH are used as separate images. The second setup we term \textbf{RGB composite data} and combined images are created by assigning VV to the red, VH to the green and the difference VV-VH to the blue channel. Note that in the second setup we end up with half the size of the single-channel dataset.


\begin{wraptable}{l}{8.5cm}
\caption{Results on single-channel and RGB composite datasets. We point in bold when the results on one of datasets are significantly better than on the other.}

\begin{tabular}{ l l r r r r r r }
 \hline
	Model & \multicolumn{2}{c}{DeepLab-V3}  & \multicolumn{2}{c}{FC-DenseNet}   \\
    Dataset & single & RGB & single & RGB \\
  \hline			
  Precision & .81 & .87 & .88  & .80 \\
  F1 score & .84 & .81 & .82 & .83 \\
  Mean IoU & .33 & .32 & .32 & .31 \\
  Forest & .95 & .94 & .93 & .94 \\
  Water & .58 & .58 & .49 & .58 \\
  Swamp &  \textbf{.25} & .11 &  \textbf{.24} & .15 \\
  Built.\ env. & .09 &  \textbf{.18} & \textbf{.22} & .10  \\
  Agriculture & .15 &  \textbf{.29} & .36 & \textbf{.60}  \\
  \hline  
\end{tabular}

\label{tab:results_stats}
\end{wraptable}

\begin{figure}
    \centering % <-- added
\begin{subfigure}{0.19\textwidth}  \caption{single-channel VV}
  \label{fig:1}
  \includegraphics[width=\linewidth]{fig/1}
\end{subfigure}\hfil % <-- added
\begin{subfigure}{0.19\textwidth}  \caption{pred on VV}
  \label{fig:2}
  \includegraphics[width=\linewidth]{fig/1_pred}
\end{subfigure}\hfil % <-- added
\begin{subfigure}{0.19\textwidth}
  \caption{label (Corine)}
  \label{fig:3}
  \includegraphics[width=\linewidth]{fig/1_gt}
\end{subfigure}
\begin{subfigure}{0.19\textwidth}  \caption{RGB composite}
  \label{fig:4}
  \includegraphics[width=\linewidth]{fig/1_rgb}
\end{subfigure}\hfil % <-- added
\begin{subfigure}{0.19\textwidth}
  \caption{pred on RGB}
  \label{fig:5}
  \includegraphics[width=\linewidth]{fig/1_rgb_pred}
\end{subfigure}

\medskip
\begin{subfigure}{0.19\textwidth}
  \includegraphics[width=\linewidth]{fig/2}
\end{subfigure}\hfil % <-- added
\begin{subfigure}{0.19\textwidth}
  \includegraphics[width=\linewidth]{fig/2_pred}
\end{subfigure}\hfil % <-- added
\begin{subfigure}{0.19\textwidth}
  \includegraphics[width=\linewidth]{fig/2_gt}
\end{subfigure}
\begin{subfigure}{0.19\textwidth}
  \includegraphics[width=\linewidth]{fig/2_rgb}
\end{subfigure}\hfil % <-- added
\begin{subfigure}{0.19\textwidth}
  \includegraphics[width=\linewidth]{fig/2_rgb_pred}
\end{subfigure}

\medskip
\begin{subfigure}{0.19\textwidth}
  \includegraphics[width=\linewidth]{fig/3}
\end{subfigure}\hfil % <-- added
\begin{subfigure}{0.19\textwidth}
  \includegraphics[width=\linewidth]{fig/3_pred}
\end{subfigure}\hfil % <-- added
\begin{subfigure}{0.19\textwidth}
  \includegraphics[width=\linewidth]{fig/3_gt}
\end{subfigure}
\begin{subfigure}{0.19\textwidth}
  \includegraphics[width=\linewidth]{fig/3_rgb}
\end{subfigure}\hfil % <-- added
\begin{subfigure}{0.19\textwidth}
  \includegraphics[width=\linewidth]{fig/3_rgb_pred}
\end{subfigure}

\medskip
\caption{\textbf{DeepLab-V3 results}: (a) single-channel VV images, (d) RGB composite (VV, VH, VV-VH), (c) Corine mask labels, and (b) and (e) corresponding segmentation outputs on (a) and (d), respectively. }
\label{fig:example_res_images}
\end{figure}

\paragraph{Preliminary results}
We use the open-source Semantic Segmentation Suite\footnote{https://github.com/GeorgeSeif/Semantic-Segmentation-Suite} to test several state-of-the-art \textit{pre-trained} semantic segmentation DL models \cite{garcia2017review} including DeepLab \cite{chen2018deeplab}, FC-DenseNet \cite{jegou2017one}, Global Convolutional Network \cite{peng2017large}, RefineNet \cite{lin2017refinenet}, and ICNet \cite{zhao2017icnet}. Hence, we apply \textit{transfer learning}.
Preliminary results for two of the models: DeepLab-V3-Res152 and FC-DenseNet-Res152 are presented in Table  
\ref{tab:results_stats}. The training with the other models is currently ongoing. Example images with segmentation output of the DeepLab-V3 model are also shown in Fig.\ \ref{fig:example_res_images}. 

There are a couple of points to note from the preliminary results. First, we can see how the algorithms perform better on single-polarization dataset when it comes to detecting swamps compared to RGB composite dataset on which the swamp class is incorrectly identified as the water class. On the other hand,  RGB composite dataset results in better classification of the agriculture class using both algorithms. For the built environment class, the results are inconsistent.

\paragraph{Summary and future work}
Our work indicates the potential for applying existing DL models to SAR image semantic segmentation. However, there is still the need for improved precision compared to traditional established methods in the SAR image analytics. Future lines of work towards that goal include better data preprocessing, adapting existing DL models to SAR data and using SAR data with a better resolution, such as from upcoming ICEYE X-band satellite constellation. 

\begin{comment}
%\captionsetup{justification=raggedleft}

\ffigbox[\textwidth]{%

\begin{subfloatrow}[5]
\ffigbox[\FBwidth]{}
  {\includegraphics[width=.18\textwidth]{fig/1}}
\ffigbox[\FBwidth]{}
  {\includegraphics[width=.18\textwidth]{fig/1_pred}}
\ffigbox[\FBwidth]{}
  {\includegraphics[width=.18\textwidth]{fig/1_gt}}
\ffigbox[\FBwidth]{}
  {\includegraphics[width=.18\textwidth]{fig/1_rgb}}
\ffigbox[\FBwidth]{}
  {\includegraphics[width=.18\textwidth]{fig/1_rgb_pred}}
\end{subfloatrow}
\vspace{1em}

\begin{subfloatrow}[5]
\ffigbox[\FBwidth]{}
  {\includegraphics[width=.18\textwidth]{fig/2}}
\ffigbox[\FBwidth]{}
  {\includegraphics[width=.18\textwidth]{fig/2_pred}}
\ffigbox[\FBwidth]{}
  {\includegraphics[width=.18\textwidth]{fig/2_gt}}
\ffigbox[\FBwidth]{}
  {\includegraphics[width=.18\textwidth]{fig/2_rgb}}
\ffigbox[\FBwidth]{}
  {\includegraphics[width=.18\textwidth]{fig/2_rgb_pred}}
\end{subfloatrow}
\vspace{1em}

\begin{subfloatrow}[5]
\ffigbox[\FBwidth]{\caption{first subfigure}\label{sfig:a}}
  {\includegraphics[width=.18\textwidth]{fig/3}}
\ffigbox[\FBwidth]{\caption{second subfigure}\label{sfig:b}}
  {\includegraphics[width=.18\textwidth]{fig/3_pred}}
\ffigbox[\FBwidth]{\caption{third subfigure}\label{sfig:c}}
  {\includegraphics[width=.18\textwidth]{fig/3_gt}}
\ffigbox[\FBwidth]{\caption{fourth subfigure}\label{sfig:d}}
  {\includegraphics[width=.18\textwidth]{fig/3_rgb}}
\ffigbox[\FBwidth]{\caption{fifth subfigure}\label{sfig:e}}
  {\includegraphics[width=.18\textwidth]{fig/3_rgb_pred}}
\end{subfloatrow}
\vspace{1em}
}

\end{comment}
\begin{comment}
\begin{floatrow}
\capbtabbox{%
\begin{tabular}{ l l r r r r }
 \hline
	Model & DeepLab-V3 & FC-DenseNet  \\
  \hline			
  Precision & 0.81 & 0.88  \\
  F1 score & 0.84 & 0.82  \\
  Mean IoU & 0.33 &  0.32  \\
  Forest & 0.95 & 0.93  \\
  Water & 0.58 & 0.49 \\
  Built.\ env. & 0.09 & \textbf{0.22}  \\
  Agriculture & 0.15 & 0.36  \\
  \hline  
\end{tabular}
 }
{%
  \caption{Results after $200$ epochs on \textit{separate} VV and VH channels}%
  \label{tab:res_grayscale}
}
\capbtabbox{%
\begin{tabular}{ l l r r r r }
 \hline
	Model & DeepLab-V3 & FC-DenseNet \\
  \hline			
  Precision & 0.87 & 0.80  \\
  F1 score & 0.81 & 0.83  \\
  Mean IoU & 0.32 &  0.31  \\
  Forest & 0.94 & 0.94  \\
  Water & 0.58 & 0.58  \\
  Built.\ env. & 0.18 & 0.10 \\
  Agriculture & 0.29 & \textbf{0.60}  \\
  \hline  
\end{tabular}
 }
{%
  \caption{Results after $200$ epochs on \textit{combined} VV and VH channels}%
    \label{tab:res_rgb}
}
\end{floatrow}
\end{comment}

\bibliographystyle{plainnat}
{\footnotesize
\bibliography{the.bib}
}
\end{document}